{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85KAhRO9kMAd"
   },
   "source": [
    "# Nested H-score: Computing Maximal Correlation Functions with Deep Learning\n",
    "\n",
    "This short script illustrates how to extract maximal correlation functions with deep learning, based on the **nested H-score**. \n",
    "\n",
    "For convenience, we will use a sythesized discrete dataset, and compare the learned features with the theoretical maximal correlation functions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8tXdzCHuGUQ"
   },
   "source": [
    "## 1. Theoretical Values of Maximal Correlation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLD5YCNNnRrX"
   },
   "source": [
    "\n",
    "We first define the generating law $P_{XY}$ of our data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 367,
     "status": "ok",
     "timestamp": 1650911087996,
     "user": {
      "displayName": "Xiangxiang Xu",
      "userId": "11233925287697466983"
     },
     "user_tz": 240
    },
    "id": "wJoft92EkGkJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# cardinalities           \n",
    "x_card, y_card = (8, 6)\n",
    "\n",
    "# randomly pick joint distribution, normalize\n",
    "p_xy = np.random.random([y_card, x_card])\n",
    "p_xy = p_xy / np.sum(p_xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8gCPBvcm1O3"
   },
   "source": [
    "Before our experiemnts in training some networks, we will define an oracle class, which is a toolbox that provides us the theoretical prediction of training results --- including learned featurs, and converged value of losses.\n",
    "\n",
    "Here the python class is used to make sure all the theoretical/underlying parameters can be called only through the toolbox. The variable p_xy will be \"released(deleted)\" to prevent cheating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1650911088224,
     "user": {
      "displayName": "Xiangxiang Xu",
      "userId": "11233925287697466983"
     },
     "user_tz": 240
    },
    "id": "gMnc_J6HL4WC"
   },
   "outputs": [],
   "source": [
    "class oracle:\n",
    "    def get_margins(self, p_mat):\n",
    "        return np.sum(p_mat, axis = 0), np.sum(p_mat, axis = 1)\n",
    "    def p2bt(self, p_mat):\n",
    "        p_x, p_y = self.get_margins(p_mat)\n",
    "        p_x = p_x.reshape(1, -1)\n",
    "        p_y = p_y.reshape(-1, 1)\n",
    "        bt_mat = p_mat / np.sqrt(p_x) / np.sqrt(p_y) - np.sqrt(p_y) * np.sqrt(p_x)\n",
    "        bt_mat[np.isnan(bt_mat)] = 0\n",
    "        return bt_mat\n",
    "    def loss(self, dim):\n",
    "        # predict the theretical value of loss\n",
    "        w_vec = dim - np.array(range(dim))\n",
    "        val = - np.sum(w_vec * (self.sigma[:dim] ** 2))\n",
    "        val = val / 2\n",
    "        return val\n",
    "    def __init__(self, p_mat):\n",
    "        self.p_xy = p_mat\n",
    "        # compute marginals\n",
    "        self.p_x, self.p_y = self.get_margins(p_xy)\n",
    "        self.Bt = self.p2bt(p_xy)\n",
    "        '''\n",
    "        Compute oracleal answers for f and g, corresponding to the 1st pair\n",
    "        of singular vectors of B\n",
    "        '''\n",
    "        Psi, self.sigma, Phi = np.linalg.svd(self.Bt)\n",
    "        Phi = Phi.T\n",
    "        self.f = Phi / np.sqrt(self.p_x).reshape(-1, 1)\n",
    "        self.g = Psi / np.sqrt(self.p_y).reshape(-1, 1)\n",
    "\n",
    "orc = oracle(p_xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZD4oz6fVIKC9"
   },
   "source": [
    "The toolbox basically computes a matrix $\\tilde{B}$ (known as the canonical dependency matrix, CDM) with entries\n",
    "$$\\tilde{B}(y, x) = \\frac{P_{XY}(x, y) - P_X(x)P_Y(y)}{\\sqrt{P_X(x)P_Y(y)}}$$\n",
    "and its SVD\n",
    "$$\\tilde{B}(y, x) = \\sum_{i = 1}^K \\sigma_i \\psi_i^*(y) \\phi_i^*(x).$$\n",
    "\n",
    "Then, the maximal correlation functions $f_i^*, g_i^*$ of $(X, Y)$ can be represented as\n",
    "$$ \n",
    "f_i^*(x) = \\frac{\\phi_i^*(x)}{\\sqrt{P_X(x)}}, \\quad g_i^*(y) = \\frac{\\psi_i^*(y)}{\\sqrt{P_Y(y)}}\n",
    "$$\n",
    "and we have $\\mathbb{E}[f_i^*(X) g_i^*(Y)] = \\sigma_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-5IK2_xxddq"
   },
   "source": [
    "## 2. Learning Maximal Correlation Functions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IHxUUgNy2rv"
   },
   "source": [
    "### 2.1 Generate Samples\n",
    "\n",
    "Then we generate samples according to $P_{XY}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1650911088225,
     "user": {
      "displayName": "Xiangxiang Xu",
      "userId": "11233925287697466983"
     },
     "user_tz": 240
    },
    "id": "7HLyKrIunQcr"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Generate random discrete samples\n",
    "\"\"\"\n",
    "def GenerateDiscreteSamples(p_xy, nSamples):\n",
    "    '''    \n",
    "    generate n samples of (X, Y) pairs, with distribution p_xy\n",
    "    \n",
    "    return a list of two np.array(dtype=int), each of size n\n",
    "    values in range(cX) and range(cY)\n",
    "    \n",
    "    as i.i.d. sample of (X,Y) pairs with joint distribution randomly chosen\n",
    "    Input:    p_xy, a cY x cX matrix\n",
    "\n",
    "    '''        \n",
    "    (y_card, x_card) = p_xy.shape\n",
    "    p_xy_vec = p_xy.reshape(-1)  # p_xy_vec is the PMF of key = Y * cX + X \n",
    "    key = np.random.choice(range(x_card*y_card), nSamples, p=p_xy_vec)\n",
    "    \"\"\"\n",
    "    key = Y * cX + X, p_xy[Y, X] = p_xy_vec[Y * cX + X], shown as follows:\n",
    "    \n",
    "    [[       0,            1,  ...,            cX-1], \n",
    "     [      cX,       cX + 1,  ...,       cX + cX-1],\n",
    "     [     2cX,      2cX + 1,  ...,      2cX + cX-1],\n",
    "            |          |       ...,          |\n",
    "     [(cY-1)cX, (cY-1)cX + 1,  ..., (cY-1)cX + cX-1]]\n",
    "    \n",
    "    \"\"\"\n",
    "    Y = (key / x_card).astype(int)\n",
    "    X = (key % x_card)\n",
    "    \n",
    "    return([X, Y])\n",
    "\n",
    "\n",
    "nSamples = 100000\n",
    "\n",
    "[X, Y] = GenerateDiscreteSamples(p_xy, nSamples)\n",
    "\n",
    "# convert X and Y to onehot labels\n",
    "x_train = np.eye(x_card)[X]\n",
    "y_train = np.eye(y_card)[Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6LwVrf1Hvit"
   },
   "source": [
    "We then delete the variable p_xy to guarantee that all information of the  joint distribution $P_{XY}$ can only be obtained from either the oracle toolbox or the training data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1650911088225,
     "user": {
      "displayName": "Xiangxiang Xu",
      "userId": "11233925287697466983"
     },
     "user_tz": 240
    },
    "id": "Gzdz7X0wHuNa"
   },
   "outputs": [],
   "source": [
    "p_xy = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVfvmcTnxmTW"
   },
   "source": [
    "Suppose we are going to extract the first 5 maximally correlated function pairs, and set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1650911088225,
     "user": {
      "displayName": "Xiangxiang Xu",
      "userId": "11233925287697466983"
     },
     "user_tz": 240
    },
    "id": "ykv11wnblLkf"
   },
   "outputs": [],
   "source": [
    "dim = 5 # dimensions of extratcted feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20YXN9i1v-RU"
   },
   "source": [
    "### 2.2 Compute Nested H-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkX4Ai6Fj-uN"
   },
   "source": [
    "Given $k$-dimensional features $f$ of $X$ and $g$ of $Y$, the H-score $\\mathscr{H}(f, g)$ is defined as\n",
    "\\begin{align*}\n",
    "   \\mathscr{H}(f, g) = \\mathbb{E}[\\langle f(X),  g(Y)\\rangle] - \\langle \\mathbb{E}[f(X)],  \\mathbb{E}[g(Y)] \\rangle - \\frac 12 \\mathrm{tr}\\left(\\mathbb{E}[f(X)f^{\\mathrm{T}}(X)]\\cdot \\mathbb{E}[g(Y)g^{\\mathrm{T}}(Y)]\\right)\n",
    "\\end{align*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1650911088226,
     "user": {
      "displayName": "Xiangxiang Xu",
      "userId": "11233925287697466983"
     },
     "user_tz": 240
    },
    "id": "Daz1lhR5pq6D"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/xiangxiang/anaconda/envs/data3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/xiangxiang/anaconda/envs/data3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/xiangxiang/anaconda/envs/data3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/xiangxiang/anaconda/envs/data3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/xiangxiang/anaconda/envs/data3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/xiangxiang/anaconda/envs/data3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/xiangxiang/anaconda/envs/data3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/xiangxiang/anaconda/envs/data3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/xiangxiang/anaconda/envs/data3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/xiangxiang/anaconda/envs/data3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/xiangxiang/anaconda/envs/data3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/xiangxiang/anaconda/envs/data3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Input, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# obtain the normalized feature by subtracting the mean\n",
    "tilde_K = lambda x: x - K.mean(x, axis = 0)\n",
    "# compute the covariance\n",
    "cov = lambda x: K.dot(K.transpose(x), x) / K.cast(K.shape(x)[0] - 1, dtype = 'float32')\n",
    "\n",
    "def neg_hscore(x):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    f, g = x\n",
    "    # subtract the mean\n",
    "    f0 = tilde_K(f)\n",
    "    g0 = tilde_K(g)\n",
    "    # compute correlation\n",
    "    corr = K.mean(K.sum(f0 * g0, axis = 1))\n",
    "    # compute covariances \n",
    "    # (indeed is E[f f.T], not centered, for extracting zero-mean features) \n",
    "    cov_f = cov(f)\n",
    "    cov_g = cov(g)\n",
    "    # negative H-score\n",
    "    neg_h = - corr + K.sum(cov_f * cov_g) / 2\n",
    "    return neg_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53SJVhSdpeaK"
   },
   "source": [
    "Then, the nested H-score is defined as\n",
    "\\begin{align*}\n",
    "   \\mathscr{H}^{\\oplus}(f, g) = \\sum_{i = 1}^k \\mathscr{H}(f^{[i]}, g^{[i]}),\n",
    "\\end{align*}\n",
    "where $f^{[i]} \\triangleq [f_1, \\dots, f_i]^\\mathrm{T}$ is the feature composed of the first $i$-dimensions of $f$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xh-SIF2Qkruv"
   },
   "source": [
    "It can be verified that the functions $f = (f_1, \\dots, f_k), g = (g_1, \\dots, g_k)$ that maximize the nested H-score would satisfy\n",
    "$$f_i = a_i \\cdot f_i^*, \\quad g_i  = b_i \\cdot g_i^*$$\n",
    "where $a_i, b_i$ are scalars with $a_i \\cdot b_i = \\sigma_i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHa3aXJm0vWO"
   },
   "source": [
    "The nested H-score $\\mathscr{H}^{\\oplus}$ can be simply computed as the cumulative sum of H-score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1650911088226,
     "user": {
      "displayName": "Xiangxiang Xu",
      "userId": "11233925287697466983"
     },
     "user_tz": 240
    },
    "id": "9aCoS9UHj53i"
   },
   "outputs": [],
   "source": [
    "def neg_hscore_nest(x):\n",
    "    \"\"\"\n",
    "    compute the (nagative) nested H-score\n",
    "    \"\"\"\n",
    "    f, g = x\n",
    "    dim = f.shape[1]\n",
    "    neg_h_nest = K.sum([neg_hscore([f[:, :i+1], g[:, :i+1]]) for i in range(dim)])\n",
    "    return neg_h_nest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08ww_e3blhHh"
   },
   "source": [
    "### 2.3 Training\n",
    "We then define our network architectures. Since both $X$ and $Y$ are discrete, it suffices to use one fully-connected layers (Dense() function in Keras) for generating features $f$ and $g$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1650911088699,
     "user": {
      "displayName": "Xiangxiang Xu",
      "userId": "11233925287697466983"
     },
     "user_tz": 240
    },
    "id": "gz_hBKxetl8k"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the learning model\n",
    "\"\"\"\n",
    "# one-hot inputs\n",
    "input_x = Input(shape = (x_card, ))\n",
    "input_y = Input(shape = (y_card, ))\n",
    "\n",
    "# using one embedding layer to generate features f and g\n",
    "# Note that they are not to be activated, as the linear layer already has full \n",
    "#         capbablity in express any feature of (one-hot) input\n",
    "f = Dense(dim)(input_x)\n",
    "g = Dense(dim)(input_y)\n",
    "\n",
    "loss = Lambda(neg_hscore_nest)([f, g])\n",
    "model = Model(inputs = [input_x, input_y], outputs = loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jklaT0slu2X"
   },
   "source": [
    "Before training the model, let's print the theoretic prediction of the converged (optimal) loss:       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1650911088701,
     "user": {
      "displayName": "Xiangxiang Xu",
      "userId": "11233925287697466983"
     },
     "user_tz": 240
    },
    "id": "odS1SRdLiPIO",
    "outputId": "7f7ecfd5-dcd8-46d2-e99a-16a54f665b05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theoretic value of loss = -0.597718057109786\n"
     ]
    }
   ],
   "source": [
    "# before the training, let's see the theoretic value of optimal loss\n",
    "print('theoretic value of loss =', orc.loss(dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7u3MXDjiG_X"
   },
   "source": [
    "We then train the model. In most cases, the model should converge within 20 epochs though I set it to 100 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 153014,
     "status": "ok",
     "timestamp": 1650911241709,
     "user": {
      "displayName": "Xiangxiang Xu",
      "userId": "11233925287697466983"
     },
     "user_tz": 240
    },
    "id": "ze5jsnLnluKI",
    "outputId": "89121059-542b-44d9-98cd-16dd74dd8665"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret optimizer identifier: <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7ff198f557d0>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ds/514pn5ld6b5f7crgw9n078gw0000gn/T/ipykernel_60780/2167696320.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/data3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;31m`\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0msample_weight_mode\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \"\"\"\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/data3/lib/python3.7/site-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    871\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m         raise ValueError('Could not interpret optimizer identifier: ' +\n\u001b[0;32m--> 873\u001b[0;31m                          str(identifier))\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: Could not interpret optimizer identifier: <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7ff198f557d0>"
     ]
    }
   ],
   "source": [
    "\"\"\" training parameters \"\"\"\n",
    "epochs = 100  \n",
    "batch_size = 128\n",
    "\n",
    "model.compile(optimizer='adam', loss = lambda y_true, y_pred: y_pred)\n",
    "\n",
    "# train the model\n",
    "hist = model.fit([x_train, y_train],\n",
    "               np.zeros([x_train.shape[0], 1]),\n",
    "               batch_size = batch_size,\n",
    "               epochs = epochs, verbose = 0)\n",
    "\n",
    "print(\"trained loss =\", hist.history['loss'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVGYgygWopS0"
   },
   "source": [
    "If you run 100 epochs as I did, the final loss value should be close to the oracle's prediction. We then extract the learned features, by feeding all possible one-hoted inputs (identity matrices) to the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 218,
     "status": "ok",
     "timestamp": 1650911241908,
     "user": {
      "displayName": "Xiangxiang Xu",
      "userId": "11233925287697466983"
     },
     "user_tz": 240
    },
    "id": "4lc6N2UMQZW7"
   },
   "outputs": [],
   "source": [
    "# define new models to obtain the trained features f and g\n",
    "model_f = Model(inputs = input_x, outputs = f)\n",
    "model_g = Model(inputs = input_y, outputs = g)\n",
    "\n",
    "f_learned = model_f.predict(np.eye(x_card))\n",
    "g_learned = model_g.predict(np.eye(y_card))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBW_VBQCnkRn"
   },
   "source": [
    "## 3. Compare with Theoretical Results\n",
    "Let's check if we have obtained the features as expected. For this purpose, we will \n",
    "1.   compute the the angles between trained features and the maximal correlation functions;\n",
    "2.   compare trained maximal correlations and theoretical ones;\n",
    "3.   compare trained maximal correlation functions with the theoretical ones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1z6GOZBymy0"
   },
   "source": [
    "### 3.1 Angles between Features\n",
    "\n",
    "First, we define the function to compute angle between vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1650911241908,
     "user": {
      "displayName": "Xiangxiang Xu",
      "userId": "11233925287697466983"
     },
     "user_tz": 240
    },
    "id": "q8JbUdTJSrdl"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "compute angle (cosine value) between two vectors\n",
    "\"\"\"\n",
    "def cos(v1, v2):\n",
    "    \"\"\"\n",
    "    Input: v1, v2\n",
    "    Output:\n",
    "                      <v1, v2>\n",
    "     cos(v1, v2) =  -------------\n",
    "                    ||v1|| ||v2||\n",
    "\n",
    "    \"\"\"\n",
    "    def ip(v1, v2): # compute inner product <v1, v2>\n",
    "        return np.sum(v1 * v2)    \n",
    "    return ip(v1, v2) / np.linalg.norm(v1) / np.linalg.norm(v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mic9lsW_nFh_"
   },
   "source": [
    "\n",
    "The angles are shown as follows, and you shall see two lists of $\\pm 0.99$'s! ðŸ˜†\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1650911241909,
     "user": {
      "displayName": "Xiangxiang Xu",
      "userId": "11233925287697466983"
     },
     "user_tz": 240
    },
    "id": "AKFTu_FbS5f5",
    "outputId": "137883a1-c0e4-4372-80e2-7b6de4d8c95a"
   },
   "outputs": [],
   "source": [
    "print('cos(f, f*) =', [cos(f_learned[:, i], orc.f[:, i]) for i in range(dim)])\n",
    "print('cos(g, g*) =', [cos(g_learned[:, i], orc.g[:, i]) for i in range(dim)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkJ0pzV1JVyV"
   },
   "source": [
    "\n",
    "Sometimes you may find there are some values not very close to 0.99, e.g., 0.90. This happens when the singular value gaps of $\\tilde{B}$ are rather small, which makes the reconstruction of singular vectors of $\\tilde{B}$ (maximal correlation functions) ill-posed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aMHvHlL2azq"
   },
   "source": [
    "### 3.2 Maximal Correlations (Singular Values of $\\tilde{B}$)\n",
    "\n",
    "Let's take a look at the singular values of $\\tilde{B}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1650911241909,
     "user": {
      "displayName": "Xiangxiang Xu",
      "userId": "11233925287697466983"
     },
     "user_tz": 240
    },
    "id": "560EQvKyJcdq",
    "outputId": "2b62d7d5-4b8a-4edb-ff0b-6bab8475f705"
   },
   "outputs": [],
   "source": [
    "print('singular values (oracle):', orc.sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLBR8Oi_Jo31"
   },
   "source": [
    "Instead of using our oracle's toolbox, we may also retrieve the singular values from our learned features, since we have \n",
    "\\begin{align*}\n",
    "\\mathbb{E}[f_i(X) g_i(Y)] = \\sigma_i \\cdot \\mathbb{E}[f_i^*(X) g_i^*(Y)]= \\sigma_i^2.\n",
    "\\end{align*}\n",
    "So it suffcies to extract features, and compute their correaltion:     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7247,
     "status": "ok",
     "timestamp": 1650911249153,
     "user": {
      "displayName": "Xiangxiang Xu",
      "userId": "11233925287697466983"
     },
     "user_tz": 240
    },
    "id": "E4eO16oGMMR-"
   },
   "outputs": [],
   "source": [
    "f_ensemble = model_f.predict(x_train)\n",
    "g_ensemble = model_g.predict(y_train)\n",
    "corr = np.mean(f_ensemble * g_ensemble, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zx8tb8gXQa62"
   },
   "source": [
    "Then, we can check if our estimated maximal correlations those obtained from oracle (theoretical predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1650911249365,
     "user": {
      "displayName": "Xiangxiang Xu",
      "userId": "11233925287697466983"
     },
     "user_tz": 240
    },
    "id": "NqvvKXhdQGWY",
    "outputId": "bf999a9e-cd47-4a76-c269-95ee4fa165cf"
   },
   "outputs": [],
   "source": [
    "# learned value of sigma\n",
    "simga = np.sqrt(corr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.title('Maximal Correlations')\n",
    "plt.xlabel('$i$')\n",
    "plt.ylabel('$\\sigma_i$')\n",
    "plt.plot(orc.sigma, 's--')\n",
    "plt.plot(simga, 'x--')\n",
    "plt.legend(['learned', 'theoretical'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxRF3rxSgljJ"
   },
   "source": [
    "## 3.3 Plot Maximal Correlation Functions\n",
    "A more straightforward way is to directly plot the features. To get rid of the annoying scaling factors, we will use the following function to normalize features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1650911249365,
     "user": {
      "displayName": "Xiangxiang Xu",
      "userId": "11233925287697466983"
     },
     "user_tz": 240
    },
    "id": "jOwdlt3khBRA"
   },
   "outputs": [],
   "source": [
    "def normalize(v):\n",
    "    \"\"\"\n",
    "    scale a vector v such that v has unit norm\n",
    "    and the first entry of v is non-negative\n",
    "    \"\"\"\n",
    "    return np.sign(v[0]) * v / np.linalg.norm(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzjfZ1cZiJ3J"
   },
   "source": [
    "Let's first take a look at the first maximal correlatin function $f_1^*(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1650911249571,
     "user": {
      "displayName": "Xiangxiang Xu",
      "userId": "11233925287697466983"
     },
     "user_tz": 240
    },
    "id": "frk_F1thUkTx",
    "outputId": "3506bc96-f0be-4d54-d52e-e0652d3d1efb"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Maximal Correlation Functions')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('$f_1^*(x)$')\n",
    "plt.plot(normalize(f_learned[:, 0]), 's--')\n",
    "plt.plot(normalize(orc.f[:, 0]), 'x--')\n",
    "plt.legend(['learned', 'theoretical'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "he10ebuMiZ_F"
   },
   "source": [
    "And here is the plot for the third maximal correlatin function  $g_3^*$ of $y$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1650911249892,
     "user": {
      "displayName": "Xiangxiang Xu",
      "userId": "11233925287697466983"
     },
     "user_tz": 240
    },
    "id": "8Rlog_YTiTX5",
    "outputId": "9eef4fd9-c4ff-41be-c077-10a1d1a2bed4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Maximal Correlation Functions')\n",
    "plt.xlabel('y')\n",
    "plt.ylabel('$g_3^*(y)$')\n",
    "plt.plot(normalize(g_learned[:, 2]), 's--')\n",
    "plt.plot(normalize(orc.g[:, 2]), 'x--')\n",
    "plt.legend(['learned', 'theoretical'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPz4ZrpcZ5/0p3VbjPl37cQ",
   "collapsed_sections": [],
   "name": "H_nest.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
